# -*- coding: utf-8 -*-
"""my Deep Convolutional Q-Learning for Pac-Man

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IzE2LfgbfyDJmssQYvMn_SfGjADMJsf

# Deep Convolutional Q-Learning for Pac-Man

## Part 0 - Installing the required packages and importing the libraries

### Installing Gymnasium
"""

!pip install gymnasium
!pip install "gymnasium[atari, accept-rom-license]"
!apt-get install -y swig
!pip install gymnasium[box2d]

"""### Importing the libraries"""

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from torch.utils.data import DataLoader, TensorDataset

"""## Part 1 - Building the AI

### Creating the architecture of the Neural Network
"""

class Network(nn.Module):
#TODO:
#1.Conv Layers
#2.Fully Connected Layers
#3.FORWARD Propagate the signal
  #from first the pacman "images" to eyes
  #to fc layers  #from the neurons #to the body of the AI
  #thats will be playing actions
  #Relu -> then Flatten
    def __init__(self, action_size,seed=42):
      #default var 42
        super(Network, self).__init__()
        #action_size:
#This is often used to define the number of possible outputs your network
#can predict or the number of actions available in an RL environment.
#(e.g., moving up, down, left, or right), action_size would be 4.***
        self.seed = torch.manual_seed(seed)
        #eyes comes first
        self.conv1=nn.Conv2d(3,32,kernel_size=8,stride=4)
        #first layer of the eyes of AI.
        #3 bc 3 RGB
        #con1 is the intance of Conv2d class
        self.bn1=nn.BatchNorm2d(32)#bc we made 32 previously
#This is a batch normalization layer. It is applied after the convolutional layer
# and helps the model to learn better and faster during training.
        self.conv2=nn.Conv2d(32,64,kernel_size=4,stride=2)
        #64: This layer generates 64 new filters.
        self.bn2=nn.BatchNorm2d(64)
        self.conv3=nn.Conv2d(64,64,kernel_size=3,stride=1)
        self.bn3=nn.BatchNorm2d(64)
        self.conv4=nn.Conv2d(64,128,kernel_size=3,stride=1)
        #3x3 piksel kernel
        #Stride(step) determines how far the kernel is shifted on the image.
        self.bn4=nn.BatchNorm2d(128)

        self.fc1=nn.Linear(10*10*128,512)
        #output 512 neurons
        #SYNTAX: self.fc = nn.Linear(input_size, output_size)

        self.fc2=nn.Linear(512,256)
        self.fc3=nn.Linear(256,action_size)
#RECAP: conv1, conv2, conv3, conv4 are the eyes of AI
#Batch normalization layers (bn1, bn2, bn3, bn4): allow better learning of each convolutional layer.
# (fc1, fc2, fc3): allow final prediction from extracted features.#
    def forward(self, state):
      #rectified= Corrected
      #relu is an activation function, commonly used in nn, that sets negative values to zero and leaves positive values unchanged
        x= F.relu(self.bn1(self.conv1(state)))
        """Non-linearity in ReLU:
        Piecewise Linear: ReLU is piecewise linear, meaning it has different linear behaviors in different regions (positive vs. negative values). This introduces non-linearity because the functionâ€™s behavior changes based on the input value.
        Thresholding: The function thresholding (setting negative values to zero) introduces a break in linearity, which helps the network to learn and represent complex patterns."""
        x= F.relu(self.bn2(self.conv2(x)))
        x= F.relu(self.bn3(self.conv3(x)))
        x= F.relu(self.bn4(self.conv4(x)))
        #FLATTEN OUR TENSOR
        x.view(x.size(0),-1)#reshape the tensor.1.dimension remains the same
        x=F.relu(self.fc1(x))
        x=F.relu(self.fc2(x))#new signal x
        x = self.fc3(x)
        return self.fc3(x)
        ##### EYES DONE ####

"""## Part 2 - Training the AI

### Setting up the environment
"""

import gymnasium as gym
env = gym.make('MsPacmanDeterministic-v0',full_action_space=False)
state_shape = env.observation_space.shape
state_size = env.observation_space.shape[0]
number_actions = env.action_space.n
print('State shape: ', state_shape)
print('State size: ', state_size)
print('Number of actions: ', number_actions)

"""### Initializing the hyperparameters"""

learning_rate = 5e-4
minibatch_size = 64
discount_factor = 0.99

"""### Preprocessing the frames"""

from PIL import Image
from torchvision import transforms
from torchvision.transforms import Resize # import Resize from torchvision.transforms


def preprocess_frame(frame):
    #frame as input  then converted to pytorch tensors
    # Convert the frame to PIL image obj
    frame = Image.fromarray(frame)#pil image obj
    preprocess=transforms.Compose([Resize((128,128)),transforms.ToTensor()])
    #resize
    return preprocess(frame).unsqueeze(0)#*
    #frame obj

"""### Implementing the DCQN class"""

class Agent():

  def __init__(self, action_size):
    self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    self.action_size = action_size
    self.local_qnetwork = Network( action_size).to(self.device)
    self.target_qnetwork = Network( action_size).to(self.device)
    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)
    self.memory = deque(maxlen=1000)

  def step(self, state, action, reward, next_state, done):
    state=preprocess_frame(state)
    """In reinforcement learning, preprocess_frame is a function used to prepare or transform the raw input
    data (such as images or frames from an environment) into a format suitable for the neural network.

    Common Preprocessing Steps
Resize:
Image resizing ensures that all input frames are of the same size. This is important because neural networks require a fixed input size. For example, resizing frames to 84x84 pixels is common in some RL tasks.
Grayscale:
Converting to grayscale can simplify the problem by reducing the complexity of the input. This is often done in environments where color information is not crucial.
Normalization:
Normalizing pixel values scales the input data to a range that the network can handle better, such as scaling pixel values from [0, 255] to [0, 1] or [-1, 1].
Channel Dimension:
Adding a channel dimension if needed, especially if the network expects a certain input shape. For instance, in some RL tasks, the input may need to be a stack of frames (e.g., four consecutive frames).
Transposing Dimensions:
Changing the order of dimensions to match the network's expected input format. For instance, converting images from (height, width, channels) to (channels, height, width). """
    next_state=preprocess_frame(next_state)
    self.memory.append((state, action, reward, next_state, done))
    #dont forget to append to memory!
    if len(self.memory) > minibatch_size:
        experiences = random.sample(self.memory,k=minibatch_size)
        """During training, rather than using the most recent experience alone,
        the agent samples a random subset (minibatch) of experiences from this
         replay memory. This helps to break the correlation between consecutive experiences
         and makes the learning process more stable.

         minibatch_size determines the number of experiences sampled from the replay memory for each training step."""
        self.learn(experiences, discount_factor)

  def act(self, state, epsilon = 0.):
    state = preprocess_frame(state).to(self.device)
    self.local_qnetwork.eval()
    with torch.no_grad():
      action_values = self.local_qnetwork(state)
    self.local_qnetwork.train()
    if random.random() > epsilon:
      return np.argmax(action_values.cpu().data.numpy())
    else:
      return random.choice(np.arange(self.action_size))

  def learn(self, experiences, discount_factor):
    states, actions, rewards,next_states, dones = zip(experiences)

    states = torch.from_numpy(np.vstack(states)).float().to(self.device)
    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)
    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)
    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)
    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)

    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)
    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)
    q_expected = self.local_qnetwork(states).gather(1, actions)
    loss = F.mse_loss(q_expected, q_targets)
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

"""### Initializing the DCQN agent"""

agent = Agent( number_actions)



"""### Training the DCQN agent"""

number_episodes = 2000
maximum_number_timesteps_per_episode = 10000
epsilon_starting_value  = 1.0
epsilon_ending_value  = 0.01
epsilon_decay_value  = 0.995
epsilon = epsilon_starting_value
scores_on_100_episodes = deque(maxlen = 100)

for episode in range(1, number_episodes + 1):
  state, _ = env.reset()
  score = 0
  for t in range(maximum_number_timesteps_per_episode):
    action = agent.act(state, epsilon)
    next_state, reward, done, _, _ = env.step(action)
    agent.step(state, action, reward, next_state, done)
    state = next_state
    score += reward
    if done:
      break
  scores_on_100_episodes.append(score)
  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)
  print('\rEpisode {}\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = "")
  if episode % 100 == 0:
    print('\rEpisode {}\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))
  if np.mean(scores_on_100_episodes) >= 500.0:
    print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))
    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')
    break

"""## Part 3 - Visualizing the results"""

import glob
import io
import base64
import imageio
from IPython.display import HTML, display
from gym.wrappers.monitoring.video_recorder import VideoRecorder

def show_video_of_model(agent, env_name):
    env = gym.make(env_name, render_mode='rgb_array')
    state, _ = env.reset()
    done = False
    frames = []
    while not done:
        frame = env.render()
        frames.append(frame)
        action = agent.act(state)
        state, reward, done, _, _ = env.step(action)
    env.close()
    imageio.mimsave('video.mp4', frames, fps=30)

show_video_of_model(agent, 'MsPacmanDeterministic-v0')

def show_video():
    mp4list = glob.glob('*.mp4')
    if len(mp4list) > 0:
        mp4 = mp4list[0]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)
        display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("Could not find video")

show_video()